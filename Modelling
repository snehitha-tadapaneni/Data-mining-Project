#%%
# Importing the necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#%%

cp_data = pd.read_csv("final_return.csv", index_col=0)


cp_data.info()
#%%
# Print first 5 rows of the dataset
cp_data.head()

# Print last 5 rows of the dataset
cp_data.tail()

#%%
# shape of our merged dataset
cp_data.shape

# Checking columns
cp_data.columns

#%%
# Statistics of the data
cp_data.describe()

# Checking the datatypes
cp_data.info()

#%%

# Checking for null values
cp_data.isnull().sum()

#%%

# Summary Statistics on Numerical Variables

cp_data.drop(columns=['census_tract']).describe()

#%%

# Data Viz

# distribution on numerical variables
num_cols = ['bathrm', 'rooms', 'kitchens', 'fireplaces', 'offense_ARSON', 'offense_ASSAULT W/DANGEROUS WEAPON', 'offense_BURGLARY', 'offense_HOMICIDE', 'offense_MOTOR VEHICLE THEFT', 'offense_ROBBERY', 'offense_SEX ABUSE', 'offense_THEFT F/AUTO', 'offense_THEFT/OTHER', 'method_GUN', 'method_KNIFE', 'method_OTHERS', 'shift_DAY', 'shift_EVENING', 'shift_MIDNIGHT']
cp_data[num_cols].hist(figsize=(10, 8), layout=(6, 4 ), edgecolor='black')
plt.suptitle('Distributions of Numerical Features')
plt.show()

# analysing the target variable - plotting a distribution to understand price
plt.figure(figsize=(10, 6))
sns.histplot(cp_data['price'], kde=True)
plt.title("Distribution of Housing Prices")
plt.xlabel("Price")
plt.ylabel("Frequency")
plt.show()



#%%

# As the price distribution is highly skewed, lets look at the outliers
# Boxplot for detecting outliers in price
plt.figure(figsize=(10, 6))
sns.boxplot(x=cp_data['price'])
plt.title("Boxplot of Housing Prices")
plt.show()


#%%

# heatmap on numerical variables
# Heatmap to understand relationship bw price and other variables
# Select only numerical columns
numerical_cols = cp_data.select_dtypes(include=['float64', 'int64']).columns
numerical_df = cp_data[numerical_cols]

# Compute the correlation matrix
corr = numerical_df.corr()

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", cbar=True)
plt.title('Correlation Heatmap')
plt.show()



# %%
cp_data['total_crimes'] = cp_data[['offense_ARSON', 'offense_ASSAULT W/DANGEROUS WEAPON', 
                                   'offense_BURGLARY', 'offense_HOMICIDE', 
                                   'offense_MOTOR VEHICLE THEFT', 'offense_ROBBERY', 'offense_SEX ABUSE', 
                                   'offense_THEFT F/AUTO', 'offense_THEFT/OTHER']].sum(axis=1)

cp_data['violent_crimes'] = cp_data[['offense_ASSAULT W/DANGEROUS WEAPON', 'offense_HOMICIDE', 'offense_ROBBERY', 'offense_SEX ABUSE']].sum(axis=1)
cp_data['non_violent_crimes'] = cp_data[['offense_ARSON','offense_BURGLARY','offense_THEFT F/AUTO', 'offense_THEFT/OTHER']].sum(axis=1)

#%%

# Heatmap on crime statistics

crime_cols = ['offense_ARSON', 'offense_ASSAULT W/DANGEROUS WEAPON', 
               'offense_BURGLARY', 'offense_HOMICIDE', 
               'offense_MOTOR VEHICLE THEFT', 'offense_ROBBERY', 'offense_SEX ABUSE', 
               'offense_THEFT F/AUTO', 'offense_THEFT/OTHER', 'price']
crime_df = cp_data[crime_cols]

# Compute the correlation matrix
corr = crime_df.corr()
corr

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", cbar=True)
plt.title('Correlation Heatmap')
plt.show()

#%%

##################################
### Target Vairable Transform ####
##################################

from scipy import stats


# Remove housing price outliers
cp_data_wo_outliers = cp_data[(np.abs(stats.zscore(cp_data)) < 2).all(axis=1)]

cp_data_wo_outliers.shape

print(cp_data_wo_outliers['price'].describe())
print(f"Skewness: {cp_data_wo_outliers['price'].skew()}")
print(f"Kurtosis: {cp_data_wo_outliers['price'].kurt()}")

sns.histplot(cp_data_wo_outliers['price'], kde=True)

#%%

# The minimum housing price is not realistic, drop the obs with housing equals to 0

cp_data_wo_extreme = cp_data[cp_data['price'] > 1000]

cp_data_wo_outliers = cp_data_wo_extreme[(np.abs(stats.zscore(cp_data)) < 2).all(axis=1)]

print(cp_data_wo_outliers['price'].describe())
print(f"Skewness: {cp_data_wo_outliers['price'].skew()}")
print(f"Kurtosis: {cp_data_wo_outliers['price'].kurt()}")

sns.histplot(cp_data_wo_outliers['price'], kde=True)
#%% 

# Standardized housing price

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
price_standardized = scaler.fit_transform(cp_data_wo_extreme['price'].values.reshape(-1, 1)).flatten()
sns.histplot(price_standardized, kde=True)


#%%

# log transform housing price
sns.histplot(np.log(cp_data_wo_extreme['price']), kde=True)
print(f"Skewness: {np.log(cp_data_wo_extreme['price']).skew()}")
print(f"Kurtosis: {np.log(cp_data_wo_extreme['price']).kurt()}")

#%%

sns.histplot(np.log(cp_data['price']), kde=True)
print(f"Skewness: {np.log(cp_data['price']).skew()}")
print(f"Kurtosis: {np.log(cp_data['price']).kurt()}")


#%%

sns.histplot(np.log(cp_data_wo_outliers['price']), kde=True)
print(f"Skewness: {np.log(cp_data_wo_outliers['price']).skew()}")
print(f"Kurtosis: {np.log(cp_data_wo_outliers['price']).kurt()}")

#%%

def plot_crime_vs_housing(df, crime_cols, target='price'):

    for crime in crime_cols:
        plt.figure(figsize=(10, 6))
        sns.lmplot(x=crime, y=target, data=df, height=6, aspect=1.5, scatter_kws={'alpha':0.5})
        sns.regplot(x=crime, y=target, data=df, scatter_kws={'alpha': 0.4},  # Make the scatter points more transparent
line_kws={'color': 'red', 'linewidth': 3} )
        plt.xlabel('Crime Counts (per Census Tract)')
        plt.ylabel('Housing Price')
        plt.title(f'Regression of Housing Prices on {crime}')
        plt.show()



crime_cols = [
    'offense_ARSON', 'offense_ASSAULT W/DANGEROUS WEAPON', 'offense_BURGLARY', 
    'offense_HOMICIDE', 'offense_MOTOR VEHICLE THEFT', 'offense_ROBBERY', 
    'offense_SEX ABUSE', 'offense_THEFT F/AUTO', 'offense_THEFT/OTHER', 
    'method_GUN', 'method_KNIFE', 'method_OTHERS', 
    'shift_DAY', 'shift_EVENING', 'shift_MIDNIGHT',
    'total_crimes', 'violent_crimes', 'non_violent_crimes'
    ]


plot_crime_vs_housing(cp_data_wo_outliers, crime_cols)

#%%

#######################
##### Modeling ########
#######################

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score


#%%
##################
##### Price ######
##################

##### All Features ####
#numlist = cp_data.drop(['ln_price', 'price', 'census_tract'], axis = 1).columns.to_list()
#X = cp_data.drop(['ln_price', 'price'], axis = 1 )
#y = cp_data['price']


##### Selective Feature Sets ####
# features = ['bathrm', 'rooms', 'kitchens', 
#             'fireplaces','offense_ARSON', 'offense_ASSAULT W/DANGEROUS WEAPON',
#             'offense_BURGLARY', 'offense_HOMICIDE','offense_MOTOR VEHICLE THEFT',
#             'offense_ROBBERY', 'offense_SEX ABUSE', 'offense_THEFT F/AUTO',
#             'offense_THEFT/OTHER','year', 'census_tract']

#features = ['bathrm', 'rooms', 'kitchens','fireplaces','violent_crimes', 'non_violent_crimes','census_tract']
features = ['bathrm', 'rooms','fireplaces','method_GUN', 'method_KNIFE', 'method_OTHERS']
numlist = features[:-1]
X = cp_data_wo_outliers[features]
y = np.log(cp_data_wo_outliers['price'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

Preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numlist)
        # ('cat', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False), ['census_tract'])
    ],
    remainder='passthrough'
)


pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('model', LinearRegression())
])


pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

# Calculate Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print(f"LR Mean Squared Error: {mse}")
print(f"LR Root Mean Squared Error: {rmse}")
# Calculate R^2 Score
r2 = r2_score(y_test, y_pred)
print(f"LR R² Score: {r2}")

#%%%

# Resources: https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/ 

        
#### Check for the Assumptions on Linear Regression ######


#%%
# Calculate residuals

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

residuals = y_test - y_pred 
fitted = y_pred

plt.figure(figsize=(8,6))
sns.scatterplot(x=fitted, y=residuals)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Fitted values')
plt.ylabel('Residuals')
plt.title('Residuals vs Fitted Values')
plt.show()

 # Plot distribution of residuals
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.title('Distribution of Residuals')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.show()
    
print('Residuals should be randomly scattered around zero without any specific pattern.')


# heteroskadasticity in residuals

#%%

############# Autocorrelation ################

from statsmodels.stats.stattools import durbin_watson

dw_stat = durbin_watson(residuals)
print(f'Durbin-Watson statistic: {dw_stat}')


#%%

############## 
# Get preprocessed training data
X_train_preprocessed = pipeline.named_steps['preprocessor'].transform(X_train)

# If OneHotEncoder was used, the number of features increases. VIF requires a dataframe.
# Let's create a dataframe for VIF calculation
feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()
X_train_preprocessed

#%%

X_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, columns=feature_names)
X_train_preprocessed_df.shape
#%%

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data['feature'] = X_train_preprocessed_df.columns
vif_data['VIF'] = [variance_inflation_factor(X_train_preprocessed_df.values, i) 
                   for i in range(X_train_preprocessed_df.shape[1])]
print(vif_data)

from sklearn.linear_model import RidgeCV

pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('model', RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5))
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)

# Evaluate Performance
from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)
print(f"Ridge Regression RMSE: {rmse}")
print(f"Ridge Regression R² Score: {r2}")
#%%

def feature_target_relationship(X, y, num_features):
    for feature in num_features:
        plt.figure(figsize=(8, 5))
        sns.scatterplot(x=X[feature], y=y)
        sns.regplot(x=X[feature], y=y, scatter=False, color='red')
        plt.title(f'Relationship between {feature} and Target')
        plt.xlabel(feature)
        plt.ylabel('Price')
        plt.show()
        print(f'Check if the relationship between {feature} and target is linear.')

# Call the feature_target_relationship function
feature_target_relationship(X_train, y_train, numlist)



#%%

import statsmodels.api as sm

# Prepare the data for statsmodels
X_train_sm = Preprocessor.fit_transform(X_train)

# Add constant
X_train_sm = sm.add_constant(X_train_sm)

# Fit the model
model_sm = sm.OLS(y_train, X_train_sm).fit()

# Display the summary
print(model_sm.summary())
#%%

##### Random Forest Regressor #####

pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('model', RandomForestRegressor(n_estimators=10, random_state=101))
])

# Fit the model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)

# Calculate metrics in log space
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RF Mean Squared Error: {mse}")
print(f"RF R² Score: {r2}")
# %%

##### SVM ######

### Log Transformation

# pipeline = Pipeline([
#     ('preprocessor', Preprocessor),
#     ('model', SVR(kernel='rbf', C= 1000))
# ])

# # Fit the model
# pipeline.fit(X_train, y_train)

# # Make predictions
# y_pred = pipeline.predict(X_test)

# # Calculate metrics in log space
# mse = mean_squared_error(y_test, y_pred)
# r2 = r2_score(y_test, y_pred)

# print(f"SVM Mean Squared Error: {mse}")
# print(f"SVM R² Score: {r2}")

#%%

##################
### Log(price) ###
##################

##### LINEAR REGRESSION ######
from sklearn.compose import ColumnTransformer

##### All Features ####
numlist = cp_data.drop(['ln_price', 'price', 'census_tract'], axis = 1).columns.to_list()
X = cp_data.drop(['ln_price', 'price'], axis = 1 )
y = cp_data['ln_price']

##### Selective Feature Sets ####
features = ['bathrm', 'rooms', 'kitchens', 'fireplaces','offense_ARSON', 'offense_ASSAULT W/DANGEROUS WEAPON', 'offense_BURGLARY', 'offense_HOMICIDE','offense_MOTOR VEHICLE THEFT', 'offense_ROBBERY', 'offense_SEX ABUSE', 'offense_THEFT F/AUTO', 'offense_THEFT/OTHER','year', 'census_tract']
# #features = ['bathrm', 'rooms', 'kitchens','fireplaces','violent_crimes', 'property_crimes','census_tract']
numlist = features[:-1]

X = cp_data[features]
y = cp_data['ln_price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

Preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numlist),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), ['census_tract'])
    ],
    remainder='passthrough'
)


pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('model', LinearRegression())
])


pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

# Calculate Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print(f"LR Mean Squared Error: {mse}")
print(f"LR Root Mean Squared Error: {rmse}")
# Calculate R^2 Score
r2 = r2_score(y_test, y_pred)
adj_r2 = 1-(1-r2)*(20135-1)/(20135-194)
print(f"LR R² Score: {r2}")
print(f"LR Adjusted R² Score: {adj_r2}")
#%%

##### Random Forest Regressor #####


pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('model', RandomForestRegressor(n_estimators=10, random_state=101))
])

# Fit the model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)

# Calculate metrics in log space
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"RF Mean Squared Error: {mse}")
print(f"RF R² Score: {r2}")
# %%

##### SVM ######

### Log Transformation

pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('model', SVR(kernel='rbf', C= 1000))
])

# Fit the model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)

# Calculate metrics in log space
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"SVM Mean Squared Error: {mse}")
print(f"SVM R² Score: {r2}")
# %%

census_tract = pd.read_csv('/Users/chengyuhan/Downloads/Census_Tracts_in_2010.csv')
census_tract.columns

census_tract[['TRACT', 'ACRES']]


#%%
####################################
## Label census_tract as hot_spot ##
####################################

# Hot spot (def)? 
# High crime vs low crime

cp_data['hotspot'] = np.where(cp_data['total_crimes'] > cp_data['total_crimes'].mean(), 'High Crime', 'Low Crime')

features = ['bathrm', 'rooms', 'kitchens', 'census_tract', 'hotspot']


cp_data['hotspot']


#%%
numlist = features[:-1]

X = cp_data[features]
y = cp_data['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

Preprocessor = ColumnTransformer(
    transformers=[
        ('num_feat', StandardScaler(), numlist),
        ('cat_feat', OrdinalEncoder(), ['hotspot'])
        # ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), ['census_tract'])
    ],
    remainder='passthrough'
)

pipeline = Pipeline([
    ('preprocessor', Preprocessor),
    ('model', LinearRegression())
])

# Fit the model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)

# Calculate metrics in log space
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
mape = mean_absolute_percentage_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"SVM Mean Squared Error: {mse}")
print(f"SVM Mean Absolute Error: {mae}")
print(f"SVM Mean Absolute Percentage Error: {mape}")
print(f"SVM R² Score: {r2}")


#%%

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

residuals = y_test - y_pred 
fitted = y_pred

plt.figure(figsize=(8,6))
sns.scatterplot(x=fitted, y=residuals)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Fitted values')
plt.ylabel('Residuals')
plt.title('Residuals vs Fitted Values')
plt.show()

 # Plot distribution of residuals
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.title('Distribution of Residuals')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.show()


#%%

############# Autocorrelation ################

from statsmodels.stats.stattools import durbin_watson

dw_stat = durbin_watson(residuals)
print(f'Durbin-Watson statistic: {dw_stat}')
# %%

############## 
# Get preprocessed training data
X_train_preprocessed = pipeline.named_steps['preprocessor'].transform(X_train)

# If OneHotEncoder was used, the number of features increases. VIF requires a dataframe.
# Let's create a dataframe for VIF calculation
feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()
X_train_preprocessed

#%%

X_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, columns=feature_names)
X_train_preprocessed_df.shape
#%%

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data['feature'] = X_train_preprocessed_df.columns
vif_data['VIF'] = [variance_inflation_factor(X_train_preprocessed_df.values, i) 
                   for i in range(X_train_preprocessed_df.shape[1])]
print(vif_data)
# %%
